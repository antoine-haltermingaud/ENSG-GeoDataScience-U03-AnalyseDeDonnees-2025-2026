{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIxuri9C088Y"
      },
      "source": [
        "# Classification of Tree Species with Machine Learning\n",
        "\n",
        "$\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad$<img src=\"https://drive.google.com/uc?id=1F4B80Q7XFcMK9aDKXGYRKgKw7q8sRP5i\" alt=\"Cache la Poudre\"  width=\"300\"/>\n",
        "<center>Cache la Poudre Natural Reserve</center>\n",
        "\n",
        "We will work in this practical on classifying tree species using topographic features, with the Cache La Poudre Natural Reserve dataset provided by *Blackard, Jock A. 1998. \"Comparison of Neural Networks and Discriminant Analysis in Predicting Forest Cover Types.\" Ph.D. dissertation. Department of Forest Sciences. Colorado State University. Fort Collins, Colorado.*\n",
        "\n",
        "\n",
        "The objective of this exercise is to test a series of machine learning algorithms for the classification of tree species. The dataset is comprised of $30\\times30$m parcels, which we want to classify among the following species:\n",
        "\n",
        "> **Class Index** | **Class Name**\n",
        "> --- | ---\n",
        "> 0 | Spruce/fir\n",
        "> 1  | Lodgepole pine\n",
        "> 2  | Ponderosa pine\n",
        "> 3  | Cottonwood/willow\n",
        "> 4  | Aspen poplar\n",
        "> 5  | Douglas-fir\n",
        "> 6  | Krummholz\n",
        "\n",
        "For each parcels, we have 10 handcrafted descriptors designed by experts. they are as follow:\n",
        "\n",
        "\n",
        "> **Index** | **Unit** | Description\n",
        "> --- | --- | ---\n",
        "> 0 | meter | Elevation\n",
        "> 1 | degree | Azimuth\n",
        "> 2 | degree | Slope\n",
        "> 3 | meter | Distance (horizontal) to nearest water body\n",
        "> 4 | meter | Distance (vertical) to nearest water body\n",
        "> 5 | meter | Distance (horizontal) to nearest road\n",
        "> 6 | index (0-255) | Shadow at 9am\n",
        "> 7 | index (0-255) | Shadow at 12am\n",
        "> 8 | index (0-255) | Shadow at 3pm\n",
        "> 9 | meter | Distance (horizontal) to nearest known fire outbreak\n",
        "\n",
        "\n",
        "All the code that needs to be completed is marked down with `#TODO`. Make sure you complete them all before moving on to the next question. Do not hesitate to create new cells and copy some code to play around with the tensors and check their size, it can be difficult to debug a notebook otherwise. You will be required to look up some documentations on numpy and sklearn online.\n",
        "\n",
        "We have added some `assert` after some cells, make sure that they pass without error before moving to the next question.\n",
        "\n",
        "## Installation and Data Download\n",
        "\n",
        "**Q1**\n",
        "Execute the cell [1] and [2], click the authentication link, copy the key and paste it in the box. Execute the data loader cell [3]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLjRBChu09S6"
      },
      "source": [
        "#[1] import and installations\n",
        "!pip install ucimlrepo\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSO23Rj4Eqyk"
      },
      "source": [
        "#[3] building the train and test sets\n",
        "data_file = fetch_ucirepo(id=31)\n",
        "features_names = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']\n",
        "data = np.array(data_file['data']['features'][features_names])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(data_file['data']['targets'].columns)\n",
        "label = np.array(data_file['data']['targets']['Cover_Type'][:]).squeeze()\n",
        "class_names = [\"Spruce/Fir\", \"Lodgepole Pine\", \"Ponderosa Pine\", \"Cottonwood/Willow\", \"Aspen Poplar\", \"Douglas-Fir\", \"Krummholz\"]\n",
        "\n",
        "# work only on a sample of the data\n",
        "perm = np.random.permutation(data.shape[0])\n",
        "data = data[perm[0:10000],:]\n",
        "label = label[perm[0:10000]]-1"
      ],
      "metadata": {
        "id": "lknBTGjXEvKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvzGuHRP-LVG"
      },
      "source": [
        "**Q2** Execute cell [4]. What can we say about the species repartition? What is the risk for the classification results if we don't remedy this issue?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOCgCZSbv76_"
      },
      "source": [
        "#[4]\n",
        "bincount = plt.hist(label, bins=[0,1,2,3,4,5,6])\n",
        "print('\\n'.join('{:20s} : {:5.0f}'.format(name, binc) for name, binc in zip(class_names,bincount[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrSuMXEZ-bCQ"
      },
      "source": [
        "**Q3** Complete cell [5] to normalize the input values to have zero mean and std one. Why is it not necessary to scale the last 4 features?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hheSfgu_Fdq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf53c661-d7d3-43b6-c7a8-80e5ebe1406b"
      },
      "source": [
        "print(data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxrIJXJREq03"
      },
      "source": [
        "#[5]\n",
        "def scale_data(x):\n",
        "  return (x - #TODO)/#TODO\n",
        "data = np.apply_along_axis(scale_data,0,data)\n",
        "assert((np.abs(np.mean(data,0))<1e-3).all())\n",
        "assert((np.abs(data[:,0:10].std(0))-1<1e-4).all())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dEl1oRi-roN"
      },
      "source": [
        "## Loss Function and Metrics\n",
        "\n",
        "We consider the two following metrics, defined with respect to the confusion matrix $CM$ of size $k \\times k$:\n",
        "\n",
        "**Overall Accuracy.** A global metric defined as the ratio of correct prediction divided by the number of (annotated) points\n",
        "    $$\n",
        "    OA = \\frac{\\text{Trace}(CM)}\n",
        "    {\\text{Sum}(CM)}.\n",
        "    $$\n",
        "**Class IoU.** This per-class metric is defined as the ratio between true positives divided by the sum of false positives, false negatives and true positives.\n",
        "    $$\n",
        "    IoU_i = \\frac{CM_{i,i}}\n",
        "    {CM_{i,i} + \\sum_{j \\neq i}\\left(CM_{i,j} + CM_{j,i} \\right)}\n",
        "    .\n",
        "    $$\n",
        "\n",
        "**Q4** Complete cell [6] for the computation of OA and IoU.\n",
        "Run the cell [7] and make sure you obtain the following:\n",
        "\n",
        "`OA = 71.43% `\n",
        "\n",
        "`Spruce/fir : 60.00% `\n",
        "\n",
        "`lodgepole pine : 25.00% `\n",
        "\n",
        "`Ponderosa pine : 100.00% `\n",
        "\n",
        "`Cottonwood/willow : 100.00% `\n",
        "\n",
        "`Aspen poplar : 100.00% `\n",
        "\n",
        "`Douglas-fir : 66.67%`\n",
        "\n",
        "`Krummholz : 33.33%`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD4GBSH7Eq3O"
      },
      "source": [
        "#[6]\n",
        "class ConfusionMatrix:\n",
        "  def __init__(self, n_class, class_names):\n",
        "    self.CM = np.zeros((n_class, n_class))\n",
        "    self.n_class = n_class\n",
        "    self.class_names = class_names\n",
        "\n",
        "  def clear(self): #reset the CM to 0\n",
        "    self.CM = np.zeros((self.n_class, self.n_class))\n",
        "\n",
        "  def add(self, gt, pred): #add a set of prediction pred with respective gt\n",
        "    \"\"\"\n",
        "    gt : vector of size N\n",
        "    pred: vector of size N\n",
        "    \"\"\"\n",
        "    self.CM +=  confusion_matrix(gt, pred, labels = list(range(self.n_class)))\n",
        "\n",
        "  def overall_accuracy(self):#percentage of correct classification\n",
        "    return 100*#TODO use HINT: np.trace\n",
        "\n",
        "  def class_IoU(self, show = 1):\n",
        "    ious = np.full(self.n_class, 0.)\n",
        "    for i_class in range(self.n_class): #this actually can be done without a loop!\n",
        "      ious[i_class] = #TODO HINT: use np.diag,  np.sum()\n",
        "    if show:\n",
        "      print(' | '.join('{} : {:3.2f}%'.format(name, 100*iou) for name, iou in zip(self.class_names,ious)))\n",
        "    #do not count classes that are not present in the dataset in the mean IoU\n",
        "    return 100*np.nansum(ious) / (np.logical_not(np.isnan(ious))).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LnhBruQywX-"
      },
      "source": [
        "#[7]\n",
        "m = ConfusionMatrix(7, class_names)\n",
        "m.add(np.array([0,1,1,5,2,0,0,4,0,5,3,6,5,1]), np.array([0,1,0,5,2,0,1,4,0,5,3,6,6,6]))\n",
        "print(m.CM)\n",
        "print(\"OA = %3.2f%%\" % (m.overall_accuracy()))\n",
        "m.class_IoU()\n",
        "m.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w90yqOYR_bG6"
      },
      "source": [
        "**Q5** Execute cell [8] to perform the train/test split. Explain briefly the role of these two datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkDrHQtVEq6w"
      },
      "source": [
        "#[8]\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm5CbOXc_iw_"
      },
      "source": [
        "## SVM Classifier\n",
        "\n",
        "**Q6** Execute cell [9] to train and evaluate a linear SVM. Read the online doc and try the following kernels: `'rbf', 'poly', 'sigmoid'`. Which one perform better? Set `class_weight=None`, and comment the effect on the mIoU and the OA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK5lKiXSEq9B"
      },
      "source": [
        "#[9]\n",
        "svm_classifier = svm.SVC(kernel='linear', C=100, class_weight='balanced', gamma = 'scale').fit(X_train, y_train)\n",
        "pred_svm = svm_classifier.predict(X_test)\n",
        "m.clear()\n",
        "m.add(y_test, pred_svm)\n",
        "print(\"OA = %3.2f%%\" % (m.overall_accuracy()))\n",
        "print(\"mIoU = %3.2f%%\" % (m.class_IoU()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AMa8pw5_2nw"
      },
      "source": [
        "**Q7** Complete cell [10] to evaluate the performance of all polynomial kernel degree 1 to 10. What is the optimal kernel size? Why is the performance dropping if the degree is too small or too large?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkhHhY2NJgvn"
      },
      "source": [
        "#[10]\n",
        "deg_array = list(range(1,10)) #list of all degrees we will try\n",
        "n_try = len(deg_array)\n",
        "mIoU_array = np.zeros((n_try,)) #array storing the miou for each configuration\n",
        "for i in range(n_try):\n",
        "  svm_classifier = svm.SVC(kernel='poly', degree=#TODO #set the SVM parameters\n",
        "  pred_svm = #TODO #prediction with SVN above\n",
        "  m.clear()\n",
        "  m.add(#TODO #put the prediction into a confusion matrix\n",
        "  mIoU_array[i] = #TODO #store the miou in the array\n",
        "  print(\"%i / %i : deg = %4.2f -> mIoU = %2.1f\" % (i, n_try-1, deg_array[i], mIoU_array[i]))\n",
        "plt.plot(deg_array, mIoU_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8** Complete cell [11] to find the optimal value of $C$ for a polynomial kernel of the degree of 3.\n",
        "Run your best model in cell [12]"
      ],
      "metadata": {
        "id": "Z_6ayxUnBgCg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TurlTRAZEbeW"
      },
      "source": [
        "#[11]\n",
        "C_array = [1,5,10,20,50,200,500,1000,2000,5000]\n",
        "#TODO\n",
        "for i in range(n_try):\n",
        "  svm_classifier = #TODO\n",
        "  #TODO: fill mIoU_array\n",
        "  print(\"%i / %i : C = %4.2f -> mIoU = %2.1f\" % (i, n_try-1, C_array[i], mIoU_array[i]))\n",
        "plt.plot(C_array, mIoU_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wm8GvvTrL5mq"
      },
      "source": [
        "#[12]\n",
        "svm_classifier = svm.SVC(#TODO - best model\n",
        "pred_svm = #TODO\n",
        "m.clear()\n",
        "m.add(#TODO\n",
        "print(\"OA = %3.2f%%\" % (m.overall_accuracy()))\n",
        "print(\"mIoU = %3.2f%%\" % (m.class_IoU()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwZ3O_wXBAqX"
      },
      "source": [
        "## KNN and RandomForest Classifier\n",
        "\n",
        "**Q9** In cell [13], perform the same analysis for the KNN classifier and find the optimal number of neighbors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouZo3uDrEq-u"
      },
      "source": [
        "#[13]\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=10).fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTNK0UoRBL__"
      },
      "source": [
        "**Q10** In cell [14], perform the same analysis for the Random Forest classifier and find the optimal `max_depth`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ty5Ujj-O_QF"
      },
      "source": [
        "#[14]\n",
        "rf_classifier = RandomForestClassifier(n_estimators=500, max_depth=2,random_state=0,class_weight='balanced').fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DConinSsCM4B"
      },
      "source": [
        "$\\star$ **Q11** Evaluate the influence of the number of trees in the forest (`n_estimators`), and comment on the effect of performance and runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7idkjazBURs"
      },
      "source": [
        "$\\star$ **Q12** Evaluate the importance of all inputs with your best performing forest (use the online doc). Perform the classification using the best 5 inputs and comment on the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlaMiQWYBxYO"
      },
      "source": [
        "$\\star$ **Q13** In cell [16], implement a 5 fold cross validation on the dataset using `sklearn.model_selection.cross_val_score` for your best performing model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oexxQByBxdO"
      },
      "source": [
        "$\\star\\star$ **Q14** Complete the function `consensus` in cell [15] which takes a list of prediction and perform a majority vote. Try a consensus voting between your best implementation of knn, SVM, and RF: the class predicted is the class with the most \"votes\". in case of equality, chose the best model. Comment on the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIHsymcqErA5"
      },
      "source": [
        "#[15]\n",
        "def consensus(pred_list, n_class = 7):\n",
        "  \"\"\" takes a list of prediction (vectors of size N) and perform a consensus vote\n",
        "  outputs a vector of size N with the prediction from consensus\n",
        "  \"\"\"\n",
        "  n_points = pred_list[0].shape[0]\n",
        "  pred_vote = np.zeros((n_points, n_class)) #store the votes\n",
        "  for pred in pred_list:\n",
        "    pred_vote[list(range(n_points)), #TODO] += 1 #count the votes for each element of the dataset\n",
        "  consensus_vote = #use argmax\n",
        "  return consensus_vote"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyhfMiW26e1C"
      },
      "source": [
        "#[16]\n",
        "pred_svm = #TODO - on test set\n",
        "pred_knn = #TODO - on test set\n",
        "pred_forest = #TODO - on test set\n",
        "#TODO\n",
        "pred_consensus = consensus([#TODO])\n",
        "m.clear()\n",
        "m.add(y_test, pred_consensus)\n",
        "print(\"OA = %3.2f%%\" % (m.overall_accuracy()))\n",
        "print(\"mIoU = %3.2f%%\" % (m.class_IoU()))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}